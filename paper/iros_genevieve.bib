@article{Abadi,
abstract = {TensorFlow [1] is an interface for expressing machine learn-ing algorithms, and an implementation for executing such al-gorithms. A computation expressed using TensorFlow can be executed with little or no change on a wide variety of hetero-geneous systems, ranging from mobile devices such as phones and tablets up to large-scale distributed systems of hundreds of machines and thousands of computational devices such as GPU cards. The system is flexible and can be used to express a wide variety of algorithms, including training and inference algorithms for deep neural network models, and it has been used for conducting research and for deploying machine learn-ing systems into production across more than a dozen areas of computer science and other fields, including speech recogni-tion, computer vision, robotics, information retrieval, natural language processing, geographic information extraction, and computational drug discovery. This paper describes the Ten-sorFlow interface and an implementation of that interface that we have built at Google. The TensorFlow API and a reference implementation were released as an open-source package under the Apache 2.0 license in November, 2015 and are available at www.tensorflow.org.},
author = {Abadi, Mart{\'{i}}n and Agarwal, Ashish and Barham, Paul and Brevdo, Eugene and Chen, Zhifeng and Citro, Craig and Corrado, Greg S and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Goodfellow, Ian and Harp, Andrew and Irving, Geoffrey and Isard, Michael and Jia, Yangqing and Jozefowicz, Rafal and Kaiser, Lukasz and Kudlur, Manjunath and Levenberg, Josh and Man{\'{e}}, Dan and Monga, Rajat and Moore, Sherry and Murray, Derek and Olah, Chris and Schuster, Mike and Shlens, Jonathon and Steiner, Benoit and Sutskever, Ilya and Talwar, Kunal and Tucker, Paul and Vanhoucke, Vincent and Vasudevan, Vijay and Vi{\'{e}}gas, Fernanda and Vinyals, Oriol and Warden, Pete and Wattenberg, Martin and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang and Research, Google},
file = {:home/genevieve/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Abadi et al. - Unknown - TensorFlow Large-Scale Machine Learning on Heterogeneous Distributed Systems.pdf:pdf},
title = {{TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems}}
}
@article{Arslan,
abstract = {— We propose a machine learning (ML)-inspired approach to estimate the relevant region of the problem during the exploration phase of sampling-based path-planners. The algorithm guides the exploration so that it draws more samples from the relevant region as the number of iterations increases. The approach works in two steps: first, it predicts if a given sample is collision-free (classification phase) without calling the collision-checker, and it then estimates if it is a promising sample, i.e., if it has the potential to improve the current best solution (regression phase), without solving the local steering problem. The proposed exploration strategy is integrated to the RRT {\#} algorithm. Numerical simulations demonstrate the efficiency of the proposed approach.},
author = {Arslan, Oktay and Tsiotras, Panagiotis},
file = {:home/genevieve/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Arslan, Tsiotras - Unknown - Machine Learning Guided Exploration for Sampling-based Motion Planning Algorithms.pdf:pdf},
title = {{Machine Learning Guided Exploration for Sampling-based Motion Planning Algorithms}}
}
@article{Blei2003,
abstract = {We describe latent Dirichlet allocation (LDA), a generative probabilistic model for collections of discrete data such as text corpora. LDA is a three-level hierarchical Bayesian model, in which each item of a collection is modeled as a finite mixture over an underlying set of topics. Each topic is, in turn, modeled as an infinite mixture over an underlying set of topic probabilities. In the context of text modeling, the topic probabilities provide an explicit representation of a document. We present efficient approximate inference techniques based on variational methods and an EM algorithm for empirical Bayes parameter estimation. We report results in document modeling, text classification, and collaborative filtering, comparing to a mixture of unigrams model and the probabilistic LSI model.},
author = {Blei, David M and Edu, Blei@cs Berkeley and Ng, Andrew Y and Edu, Ang@cs Stanford and Jordan, Michael I and Edu, Jordan@cs Berkeley},
file = {:home/genevieve/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Blei et al. - 2003 - Latent Dirichlet Allocation.pdf:pdf},
journal = {Journal of Machine Learning Research},
pages = {993--1022},
title = {{Latent Dirichlet Allocation}},
volume = {3},
year = {2003}
}
@article{Krestel,
abstract = {Tagging systems have become major infrastructures on the Web. They allow users to create tags that annotate and cat-egorize content and share them with other users, very helpful in particular for searching multimedia content. However, as tagging is not constrained by a controlled vocabulary and annotation guidelines, tags tend to be noisy and sparse. Es-pecially new resources annotated by only a few users have often rather idiosyncratic tags that do not reflect a common perspective useful for search. In this paper we introduce an approach based on Latent Dirichlet Allocation (LDA) for recommending tags of resources in order to improve search. Resources annotated by many users and thus equipped with a fairly stable and complete tag set are used to elicit la-tent topics to which new resources with only a few tags are mapped. Based on this, other tags belonging to a topic can be recommended for the new resource. Our evaluation shows that the approach achieves significantly better preci-sion and recall than the use of association rules, suggested in previous work, and also recommends more specific tags. Moreover, extending resources with these recommended tags significantly improves search for new resources.},
author = {Krestel, Ralf and Fankhauser, Peter and Nejdl, Wolfgang},
file = {:home/genevieve/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Krestel, Fankhauser, Nejdl - Unknown - Latent Dirichlet Allocation for Tag Recommendation.pdf:pdf},
keywords = {Categories and Subject Descriptors E1 [Data],Data Structures—Graphs and networks,Experimentation,H33 [Information Storage and Retrieval],I27 [Ar-tificial Intelligence],Information Search and Retrieval—Clustering,Information filtering,Measurement Keywords social bookmarking system,delicious,tag recommendation,tag search},
title = {{Latent Dirichlet Allocation for Tag Recommendation}}
}
@article{Masci,
abstract = {We present a novel convolutional auto-encoder (CAE) for unsupervised feature learning. A stack of CAEs forms a convolutional neural network (CNN). Each CAE is trained using conventional on-line gradient descent without additional regularization terms. A max-pooling layer is essential to learn biologically plausible features consistent with those found by previous approaches. Initializing a CNN with filters of a trained CAE stack yields superior performance on a digit (MNIST) and an object recognition (CIFAR10) benchmark.},
author = {Masci, Jonathan and Meier, Ueli and Cirean, Dan and Schmidhuber, J{\"{u}}rgen},
file = {:home/genevieve/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Masci et al. - Unknown - Stacked Convolutional Auto-Encoders for Hierarchical Feature Extraction.pdf:pdf},
keywords = {auto-encoder,classification,convolutional neural network,unsupervised learning},
title = {{Stacked Convolutional Auto-Encoders for Hierarchical Feature Extraction}}
}
@article{Mikolov2012,
abstract = {Recurrent neural network language models (RNNLMs) have recently demonstrated state-of-the-art performance across a variety of tasks. In this paper, we improve their performance by providing a contextual real-valued input vector in association with each word. This vector is used to convey contextual information about the sentence being modeled. By performing Latent Dirichlet Allocation using a block of preceding text, we achieve a topic-conditioned RNNLM. This approach has the key advantage of avoiding the data fragmentation associated with building multiple topic models on different data subsets. We report perplexity results on the Penn Treebank data, where we achieve a new state-of-the-art. We further apply the model to the Wall Street Journal speech recognition task, where we observe improvements in word-error-rate.},
author = {Mikolov, Tomas and Zweig, Geoffrey},
file = {:home/genevieve/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mikolov, Zweig - 2012 - Context Dependent Recurrent Neural Network Language Model.pdf:pdf},
keywords = {Language Modeling,Latent Dirichlet Allo-cation,Recurrent Neural Network,Topic Models},
title = {{Context Dependent Recurrent Neural Network Language Model}},
year = {2012}
}
@article{Ranzato,
abstract = {Finding good representations of text docu-ments is crucial in information retrieval and classification systems. Today the most pop-ular document representation is based on a vector of word counts in the document. This representation neither captures dependencies between related words, nor handles synonyms or polysemous words. In this paper, we pro-pose an algorithm to learn text document representations based on semi-supervised au-toencoders that are stacked to form a deep network. The model can be trained efficiently on partially labeled corpora, producing very compact representations of documents, while retaining as much class information and joint word statistics as possible. We show that it is advantageous to exploit even a few labeled samples during training.},
author = {Ranzato, Marc Aurelio and Szummer, Martin},
file = {:home/genevieve/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ranzato, Szummer - Unknown - Semi-supervised Learning of Compact Document Representations with Deep Networks.pdf:pdf},
title = {{Semi-supervised Learning of Compact Document Representations with Deep Networks}}
}
@article{Salakhutdinov,
abstract = {We introduce HD (or " Hierarchical-Deep ") models, a new compositional learn-ing architecture that integrates deep learning models with structured hierarchical Bayesian models. Specifically we show how we can learn a hierarchical Dirichlet process (HDP) prior over the activities of the top-level features in a Deep Boltz-mann Machine (DBM). This compound HDP-DBM model learns to learn novel concepts from very few training examples, by learning low-level generic features, high-level features that capture correlations among low-level features, and a cat-egory hierarchy for sharing priors over the high-level features that are typical of different kinds of concepts. We present efficient learning and inference algorithms for the HDP-DBM model and show that it is able to learn new concepts from very few examples on CIFAR-100 object recognition, handwritten character recogni-tion, and human motion capture datasets.},
author = {Salakhutdinov, Ruslan and Tenenbaum, Joshua B and Torralba, Antonio},
file = {:home/genevieve/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Salakhutdinov, Tenenbaum, Torralba - Unknown - Learning to Learn with Compound HD Models(2).pdf:pdf},
title = {{Learning to Learn with Compound HD Models}}
}
@inproceedings{Shen2014,
address = {New York, New York, USA},
author = {Shen, Yelong and He, Xiaodong and Gao, Jianfeng and Deng, Li and Mesnil, Gr{\'{e}}goire},
booktitle = {Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management - CIKM '14},
doi = {10.1145/2661829.2661935},
file = {:home/genevieve/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Shen et al. - 2014 - A Latent Semantic Model with Convolutional-Pooling Structure for Information Retrieval.pdf:pdf},
isbn = {9781450325981},
keywords = {convolutional neural network,deep learning,semantic representation,web search},
pages = {101--110},
publisher = {ACM Press},
title = {{A Latent Semantic Model with Convolutional-Pooling Structure for Information Retrieval}},
url = {http://dl.acm.org/citation.cfm?doid=2661829.2661935},
year = {2014}
}
@article{Steinberga,
abstract = {With the advent of cheap, high fidelity, digital imaging systems, the quantity and rate of generation of visual data can dramatically outpace a humans ability to label or an-notate it. In these situations there is scope for the use of unsupervised approaches that can model these datasets and automatically summarise their content. To this end, we present a totally unsupervised, and annotation-less, model for scene understanding. This model can simultaneously cluster whole-image and segment descriptors, thereby form-ing an unsupervised model of scenes and objects. We show that this model outperforms other unsupervised models that can only cluster one source of information (image or seg-ment) at once. We are able to compare unsupervised and su-pervised techniques using standard measures derived from confusion matrices and contingency tables. This shows that our unsupervised model is competitive with current super-vised and weakly-supervised models for scene understand-ing on standard datasets. We also demonstrate our model operating on a dataset with more than 100,000 images col-lected by an autonomous underwater vehicle.},
author = {Steinberg, Daniel M and Pizarro, Oscar and Williams, Stefan B},
doi = {10.1109/ICCV.2013.430},
file = {:home/genevieve/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Steinberg, Pizarro, Williams - Unknown - Synergistic Clustering of Image and Segment Descriptors for Unsupervised Scene Understanding(2).pdf:pdf},
title = {{Synergistic Clustering of Image and Segment Descriptors for Unsupervised Scene Understanding}}
}
@article{Steinbergb,
abstract = {For very large datasets with more than a few classes, producing ground-truth data can represent a substantial, and potentially expensive, human effort. This is particularly evident when the datasets have been collected for a particular purpose, e.g. scientific inquiry, or by autonomous agents in novel and inaccessible environments. In these situations there is scope for the use of unsuper-vised approaches that can model collections of images and automatically summarise their content. To this end, we present novel hierarchical Bayesian models for image clustering, image segment clustering, and unsupervised scene understanding. The purpose of this investigation is to highlight and compare hierarchical structures for modelling context within images based on visual data alone. We also compare the unsupervised models with state-of-the-art supervised and weakly supervised models for image under-standing. We show that some of the unsupervised models are competitive with the supervised and weakly supervised models on standard datasets. Finally, we demonstrate these unsupervised models working on a large dataset containing more than one hundred thousand images of the sea floor collected by a robot.},
author = {Steinberg, Daniel M and Pizarro, Oscar and Williams, Stefan B},
file = {:home/genevieve/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Steinberg, Pizarro, Williams - Unknown - Hierarchical Bayesian Models for Unsupervised Scene Understanding.pdf:pdf},
keywords = {Scene understanding,clustering,hierarchical Bayesian models,topic models,unsupervised learning,variational Bayes},
title = {{Hierarchical Bayesian Models for Unsupervised Scene Understanding}}
}
@article{Teh2006,
abstract = {We consider problems involving groups of data where each observation within a group is a draw from a mixture model and where it is desirable to share mixture components between groups. We assume that the number of mixture components is unknown a priori and is to be inferred from the data. In this setting it is natural to consider sets of Dirichlet processes, one for each group, where the well-known clustering property of the Dirichlet process provides a nonparametric prior for the number of mixture components within each group. Given our desire to tie the mixture models in the various groups, we consider a hierarchical model, specifically one in which the base measure for the child Dirichlet processes is itself distributed according to a Dirichlet process. Such a base measure being discrete, the child Dirichlet processes necessarily share atoms. Thus, as desired, the mixture models in the different groups necessarily share mixture components. We discuss representations of hierarchical Dirichlet processes ...},
author = {Teh, Yee Whye and Jordan, Michael I and Beal, Matthew J and Blei, David M},
doi = {10.1198/016214506000000302},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
keywords = {Clustering,Hierarchical model,Markov chain Monte Carlo,Mixture model,Nonparametric Bayesian statistics},
month = {dec},
number = {476},
pages = {1566--1581},
publisher = {Taylor {\&} Francis},
title = {{Hierarchical Dirichlet Processes}},
url = {http://www.tandfonline.com/doi/abs/10.1198/016214506000000302},
volume = {101},
year = {2006}
}
@article{Wan,
abstract = {This paper introduces a hybrid model that combines a neural network with a latent topic model. The neural network provides a low-dimensional embedding for the input data, whose subsequent distribution is captured by the topic model. The neural network thus acts as a trainable feature extractor while the topic model captures the group struc-ture of the data. Following an initial pre-training phase to separately initialize each part of the model, a unified training scheme is introduced that allows for discriminative training of the entire model. The approach is evaluated on visual data in scene classifica-tion task, where the hybrid model is shown to outperform models based solely on neu-ral networks or topic models, as well as other baseline methods.},
author = {Wan, Li and Zhu, Leo and Fergus, Rob},
file = {:home/genevieve/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wan, Zhu, Fergus - Unknown - A Hybrid Neural Network-Latent Topic Model(2).pdf:pdf},
title = {{A Hybrid Neural Network-Latent Topic Model}}
}
@article{Wang,
abstract = {Most topic models, such as latent Dirichlet allocation, rely on the bag-of-words assumption. However, word order and phrases are often critical to capturing the meaning of text in many text mining tasks. This paper presents topical n-grams, a topic model that discovers topics as well as top-ical phrases. The probabilistic model generates words in their textual order by, for each word, first sampling a topic, then sampling its status as a unigram or bigram, and then sampling the word from a topic-specific unigram or bigram distribution. Thus our model can model " white house " as a special meaning phrase in the 'politics' topic, but not in the 'real estate' topic. Successive bigrams form longer phrases. We present experimental results showing mean-ingful phrases and more interpretable topics from the NIPS data and improved information retrieval performance on a TREC collection.},
author = {Wang, Xuerui and Mccallum, Andrew and Wei, Xing},
file = {:home/genevieve/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang, Mccallum, Wei - Unknown - Topical N-grams Phrase and Topic Discovery, with an Application to Information Retrieval.pdf:pdf},
title = {{Topical N-grams: Phrase and Topic Discovery, with an Application to Information Retrieval}}
}
@article{Zhao,
abstract = {We present a novel architecture, the " stacked what-where auto-encoders " (SWWAE), which integrates discriminative and generative pathways and provides a unified approach to supervised, semi-supervised and unsupervised learning with-out relying on sampling during training. An instantiation of SWWAE uses a con-volutional net (Convnet) (LeCun et al. (1998)) to encode the input, and employs a deconvolutional net (Deconvnet) (Zeiler et al. (2010)) to produce the reconstruc-tion. The objective function includes reconstruction terms that induce the hidden states in the Deconvnet to be similar to those of the Convnet. Each pooling layer produces two sets of variables: the " what " which are fed to the next layer, and its complementary variable " where " that are fed to the corresponding layer in the generative decoder.},
author = {Zhao, Junbo and Mathieu, Michael and Goroshin, Ross and Lecun, Yann},
file = {:home/genevieve/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhao et al. - Unknown - STACKED WHAT-WHERE AUTO-ENCODERS.pdf:pdf},
title = {{STACKED WHAT-WHERE AUTO-ENCODERS}}
}
